# -*- coding: utf-8 -*-
"""Another copy of Copy of Fraud in Insurance

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FPVvMLP18b5JtxGJonOlpGdEsk690TUP
"""



"""**A PROGRAM THAT LEAVERAGES ARTIFICIAL INTELLIGENCE TO IDENTITY FRAUD IN INSURANCE CLAIMS, REDUCING COSTS FOR INSURANCE COMPANIES AND POTENTIALLY LOWERING PREMIUMS FOR CUSTOMERS.**"""



#Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

data= pd.read_csv("/content/insurance_claims.csv")
data.head()

data.replace('?', np.nan, inplace=True)

data.describe()

data.info()

data.isna().sum()

missing=data.isnull().sum()/len(data)#percentage
missing=missing[missing>0]
missing.sort_values(inplace=True)
missing=missing.to_frame()
missing.columns=['Null_Count']
missing.index.names=['Col_Name']
missing=missing.reset_index()
sns.set(style='whitegrid',color_codes=True)
sns.barplot(x='Col_Name',y='Null_Count',data=missing)
plt.xticks(rotation=90)
plt.show()

data['collision_type']=data['collision_type'].fillna(data['collision_type'].mode()[0])
data['property_damage']=data['property_damage'].fillna(data['property_damage'].mode()[0])
data['police_report_available']=data['police_report_available'].fillna(data['police_report_available'].mode()[0] )

plt.figure(figsize=(18,15))
corr=data.corr()
sns.heatmap(data=corr,annot=True,fmt='.1g',linewidth=2)
plt.show()

unique=data.nunique().to_frame()
unique.columns=['Count']
unique.index.names=['ColName']
unique=unique.reset_index()
sns.set(style='whitegrid',color_codes=True)
sns.barplot(x='ColName',y='Count',data=unique)
plt.xticks(rotation=90)
plt.show()

unique.sort_values(by='Count',ascending=False)

#Drop Columns that are not used in our project
to_drop=['policy_number','policy_bind_date','policy_state','insured_zip','incident_location','incident_date','incident_state','incident_city','insured_hobbies','auto_model','auto_year','_c39']
data.drop(to_drop,inplace=True,axis=1)

data.head()

plt.figure(figsize=(18,15))
corr=data.corr()
mask=np.triu(np.ones_like(corr,dtype=bool))
sns.heatmap(data=corr,mask=mask,annot=True,fmt='.2g',linewidth=1)
plt.show()

data.drop(columns=['age','total_claim_amount'],inplace=True,axis=1)

data.head()

#Get Target and Independent Features Seperated
x=data.drop('fraud_reported',axis=1)
y=data['fraud_reported']

#Converting Label Columns into Numerical by doing One-Encoding
categorical_cols=x.select_dtypes(include=['object'])
categorical_cols=pd.get_dummies(categorical_cols,drop_first=True)
categorical_cols.head()

numerical_col=x.select_dtypes(include=['int64'])
x=pd.concat([numerical_col,categorical_cols],axis=1)

x.head()

#Outlier Check

plt.figure(figsize=(20,15))
plotnumber=1

for col in x.columns:
  if  plotnumber<=24:
    ax=plt.subplot(7,7,plotnumber)
    sns.boxplot(x[col])
    plt.xlabel(col,fontsize=30)

    plotnumber+=1
    plt.tight_layout()
    plt.show()

"""Outliers are in there,so we need to Standardize those coloumns using Standard Scaler."""

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25)

X_train.head()



numerical_data=X_train[['months_as_customer','policy_deductable','umbrella_limit','capital-gains','capital-loss','incident_hour_of_the_day','number_of_vehicles_involved','bodily_injuries','witnesses','injury_claim','property_claim','vehicle_claim']]

#Standadization
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaled_data=scaler.fit_transform(numerical_data)

scaled_num_df=pd.DataFrame(data=scaled_data,columns=numerical_data.columns,index=X_train.index)
scaled_num_df.head()

X_train.drop(columns=scaled_num_df.columns,inplace=True)

X_train=pd.concat([scaled_num_df,X_train],axis=1)

X_train.head()

"""Modelling

Support Vector Classifier
"""

from sklearn.svm import SVC

svc_model=SVC()
svc_model.fit(X_train,y_train)

y_pred = svc_model.predict(X_test)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
svc_model_train_acc=accuracy_score(y_train,svc_model.predict(X_train))
svc_model_test_acc=accuracy_score(y_test,y_pred)

print("Training Accuracy:",svc_model_train_acc)
print("Testing Accuracy",svc_model_test_acc)
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))



"""### THANK YOU"""